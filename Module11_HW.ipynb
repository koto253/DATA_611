{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koto253/DATA_611/blob/main/Module11_HW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue2UOqaBuuEL"
      },
      "source": [
        "## Homework Week 11\n",
        "\n",
        "1. Why is it generally preferable to use a Logistic Regression classifier rather than a classical Perceptron (i.e., a single layer of threshold logic units trained using the Perceptron training algorithm)? How can you tweak a Perceptron to make it equivalent to a Logistic Regression classifier?\n",
        "\n",
        "2. Can you list all the hyperparameters you can tweak in a basic MLP? If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?\n",
        "\n",
        "3. Train a deep MLP on the MNIST dataset (you can load it using keras.datasets.mnist.load_data(). See what accuracy you can get. Try searching for the optimal learning rate. The MNIST dataset is a set of 70,000 small images of digits handwritten (0-9) by high school students and employees of the US Census Bureau. Each image is labeled with the digit it represents. There are 70,000 images, and each image has 784 features. This is because each image is 28 × 28 pixels, and each feature simply represents one pixel’s intensity, from 0 (white) to 255 (black).\n",
        "\n",
        "To check the out of sample accuracy:\n",
        "```\n",
        "model.evaluate(X_test, y_test)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Logistic Regression is like a smart way to classify things into groups, giving you the probability that something belongs to a group. It's smoother and more flexible. To make a Perceptron act like Logistic Regression, change how it decides and updates its guesses, so it also gives probabilities and improves its decisions over time. This helps both methods work similarly in predicting and categorizing stuff."
      ],
      "metadata": {
        "id": "s_7erB1mafaL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.\n",
        "    **MLP Basics:** Think of an MLP like a group of little decision-makers (neurons) working together. They learn from examples to tell things apart, like recognizing cats and dogs from pictures.\n",
        "\n",
        "    **Too Smart for Their Own Good:** Sometimes these decision-makers can get too good at remembering things, even stuff that doesn't matter. This is called overfitting, and we want to stop it.\n",
        "\n",
        "    **Fixing Overfitting:** We can try a few things to stop overfitting:\n",
        "      **Use Fewer Neurons: **Imagine having fewer decision-makers, so they can't memorize everything.\n",
        "      **Slow Down Learning:** Make the decision-makers adjust themselves more slowly to avoid learning too much too quickly.\n",
        "      **Change Their Rules:** Make the decision-makers use different ways to decide things, which might help them avoid remembering too much.\n",
        "      **Stop Early:** Keep an eye on their work and tell them to stop if they start remembering things too well.\n",
        "\n",
        "    **Experiment and Learn:** It's like a puzzle – we try different ways until the decision-makers get better at telling things apart without remembering too much. This helps us make better predictions."
      ],
      "metadata": {
        "id": "H58KQTOKa48F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries"
      ],
      "metadata": {
        "id": "UqQr7k7ybysj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "MkQiK2aWbcv0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and Preprocess Data"
      ],
      "metadata": {
        "id": "ETJwTrI2b4YB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "x_train = x_train.reshape((x_train.shape[0], 28 * 28))\n",
        "x_test = x_test.reshape((x_test.shape[0], 28 * 28))\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)"
      ],
      "metadata": {
        "id": "3ZIJZbMcbgz2",
        "outputId": "72e99801-72cb-40f1-9a78-66afc4bd4e53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Deep MLP Model"
      ],
      "metadata": {
        "id": "jMAwNZCkb85M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(784,)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "WTNo_6cubp_6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile the Model"
      ],
      "metadata": {
        "id": "0d_48kQicBZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001  # You can start with a default value and later fine-tune it\n",
        "optimizer = Adam(learning_rate=learning_rate)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "PC6OYPQebsqO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the Model"
      ],
      "metadata": {
        "id": "yVC5TueQcFxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "epochs = 10\n",
        "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
      ],
      "metadata": {
        "id": "7ax6BPCBbupt",
        "outputId": "97ebffb5-3b66-4da5-d42b-885fd383b938",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "844/844 [==============================] - 3s 3ms/step - loss: 0.2915 - accuracy: 0.9156 - val_loss: 0.1218 - val_accuracy: 0.9645\n",
            "Epoch 2/10\n",
            "844/844 [==============================] - 2s 2ms/step - loss: 0.1214 - accuracy: 0.9635 - val_loss: 0.0835 - val_accuracy: 0.9758\n",
            "Epoch 3/10\n",
            "844/844 [==============================] - 2s 2ms/step - loss: 0.0799 - accuracy: 0.9758 - val_loss: 0.0867 - val_accuracy: 0.9747\n",
            "Epoch 4/10\n",
            "844/844 [==============================] - 3s 3ms/step - loss: 0.0604 - accuracy: 0.9811 - val_loss: 0.0799 - val_accuracy: 0.9743\n",
            "Epoch 5/10\n",
            "844/844 [==============================] - 2s 2ms/step - loss: 0.0472 - accuracy: 0.9852 - val_loss: 0.0736 - val_accuracy: 0.9782\n",
            "Epoch 6/10\n",
            "844/844 [==============================] - 2s 2ms/step - loss: 0.0367 - accuracy: 0.9884 - val_loss: 0.0706 - val_accuracy: 0.9800\n",
            "Epoch 7/10\n",
            "844/844 [==============================] - 2s 2ms/step - loss: 0.0308 - accuracy: 0.9900 - val_loss: 0.0774 - val_accuracy: 0.9795\n",
            "Epoch 8/10\n",
            "844/844 [==============================] - 2s 2ms/step - loss: 0.0240 - accuracy: 0.9922 - val_loss: 0.0798 - val_accuracy: 0.9790\n",
            "Epoch 9/10\n",
            "844/844 [==============================] - 2s 2ms/step - loss: 0.0212 - accuracy: 0.9929 - val_loss: 0.0890 - val_accuracy: 0.9785\n",
            "Epoch 10/10\n",
            "844/844 [==============================] - 3s 3ms/step - loss: 0.0165 - accuracy: 0.9946 - val_loss: 0.0874 - val_accuracy: 0.9792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the Model"
      ],
      "metadata": {
        "id": "83Nr-K-AcML9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "UOVdJD1YbwWr",
        "outputId": "e6058076-fd78-4b33-92ee-08e445cdd11a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 0s 1ms/step - loss: 0.0885 - accuracy: 0.9773\n",
            "Test Accuracy: 97.73%\n"
          ]
        }
      ]
    }
  ]
}