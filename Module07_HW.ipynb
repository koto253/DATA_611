{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koto253/MATH_611/blob/main/Module07_HW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHK-G76bp0zb"
      },
      "source": [
        "## Homework Week 7\n",
        "\n",
        "1. Why the holdout method for model selection suggests to separate the data into three parts: a training set, a validation set, and a test set?\n",
        "2. Given a data set (wine), split data (20% test), apply pipeline to standardize the data, classify the data using KNeighborsClassifier (n_neighbors=10), print the test accuracy.\n",
        "\n",
        "```python\n",
        "from sklearn import datasets\n",
        "df = datasets.load_wine()\n",
        "X = df.data\n",
        "y = df.target\n",
        "```\n",
        "\n",
        "3. What is learning curve? Base on learning curve, how do you know if the model is over fitting or not?\n",
        "4. In the above data set, fit KNN using 10-fold cross validation and grid search to optimize the number of neighbors; print the optimized parameters and the test accuracy.\n",
        "5. Calculate the accuracy, precision and recall based on the following confusion matrix.\n",
        "\n",
        "|  | predicted N0 | predicted Yes|\n",
        "|--|--------------| -------------|\n",
        "|Actual No| 50 | 10|\n",
        "|Actual Yes| 5 | 100|\n",
        "\n",
        "6. Read the last section in the Chapter 6 of textbook, \"Dealing with class imbalance\". Discuss why the accuracy is not a valid meature metric in an imbalanced dataset? What other metrics can be used then?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Why the holdout method for model selection suggests to separate the data into three parts: a training set, a validation set, and a test set?**\n",
        "\n",
        "We want it to perform well not just on the data it has seen during training but also on new, unseen data. Overfitting is a situation where the model becomes too specialized in the training data and performs poorly on new data. to prevent overfitting, we split the data into three parts.\n",
        "\n",
        "It ensures that the model is not only good at remembering the training data but also at generalizing and making accurate predictions on new, unseen data. This helps us choose the best model, find the right settings, and get reliable results for real-world applications."
      ],
      "metadata": {
        "id": "4GO1_AnVlLqv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Given a data set (wine), split data (20% test), apply pipeline to standardize the data, classify the data using KNeighborsClassifier (n_neighbors=10), print the test accuracy.**"
      ],
      "metadata": {
        "id": "5eMYKAsdmoc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import datasets\n",
        "\n",
        "# Load the wine dataset\n",
        "df = datasets.load_wine()\n",
        "X = df.data\n",
        "y = df.target\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a pipeline with standardization and K-nearest neighbors classifier\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),  # Standardize the features\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=10))  # K-nearest neighbors classifier\n",
        "])\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the accuracy on the test set\n",
        "test_accuracy = pipeline.score(X_test, y_test)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(\"Test Accuracy:\", test_accuracy * 100,\"%\")"
      ],
      "metadata": {
        "id": "Bbw9TQ42mWus",
        "outputId": "9e958dcb-b47c-4095-cbce-66e6bab9d988",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 97.22222222222221 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. What is learning curve? Base on learning curve, how do you know if the model is over fitting or not?**\n",
        "\n",
        "The learning curve shows how well the model performs as more data is used. If the training and validation/test errors are both low and similar, the model is likely a good fit. If there is a large gap between them, it suggests overfitting, and adjustments like reducing complexity or getting more data may be needed."
      ],
      "metadata": {
        "id": "vzJto0C8nfGr"
      }
    }
  ]
}